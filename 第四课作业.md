[XTuner å¾®è°ƒ LLMï¼š1.8Bã€å¤šæ¨¡æ€ã€Agent](https://github.com/InternLM/Tutorial/tree/camp2/xtuner)
[XTuner å¾®è°ƒä¸ªäººå°åŠ©æ‰‹](https://github.com/InternLM/Tutorial/blob/camp2/xtuner/personal_assistant_document.md)
[XTunerå¤šæ¨¡æ€è®­ç»ƒä¸æµ‹è¯•](https://github.com/InternLM/Tutorial/blob/camp2/xtuner/personal_assistant_document.md)

# Fintune ç®€ä»‹

## ä¸¤ç§ Finetune èŒƒå¼

### å¢é‡é¢„è®­ç»ƒ
è®©åŸºåº§æ¨¡å‹ï¼ˆHugging Faceå¼€æºçš„æ¨¡å‹ï¼‰å­¦ä¹ åˆ°**æ–°çŸ¥è¯†**ï¼Œæ¯”å¦‚æŸä¸ªé¢†åŸŸçš„å¸¸è¯†
è®­ç»ƒæ•°æ®ï¼šæ–‡ç« ã€ä¹¦ç±ã€ä»£ç ç­‰
### æŒ‡ä»¤è·Ÿéšå¾®è°ƒ
è®©æ¨¡å‹å­¦ä¼šå¯¹è¯ï¼Œå¯ä»¥æ ¹æ®äººçš„æŒ‡ä»¤è¿›è¡Œå¯¹è¯
è®­ç»ƒæ•°æ®ï¼šé«˜è´¨é‡**å¯¹è¯**ï¼ˆå¿…é¡»æ˜¯ä¸€é—®ä¸€ç­”ï¼‰

![å¾®è°ƒèŒƒå¼](./imgs/å¾®è°ƒèŒƒå¼.png)

## ä¸€æ¡æ•°æ®çš„ä¸€ç”Ÿ

![å¾®è°ƒæ•°æ®å¤„ç†](./imgs/å¾®è°ƒæ•°æ®å¤„ç†.png)

### åŸå§‹æ•°æ®

![å›¾ç‰‡](./imgs/åŸå§‹æ•°æ®.png)

### æ ‡å‡†æ•°æ®æ ¼å¼

è¿™æ˜¯ InterLM çš„æ ‡å‡†æ•°æ®æ ¼å¼ï¼Œä¹Ÿæ˜¯éœ€è¦å‡†å¤‡å¥½çš„æ•°æ®æ ¼å¼

![å›¾ç‰‡](./imgs/æ ‡å‡†æ•°æ®æ ¼å¼.png)

### å¯¹è¯æ¨¡æ¿

å¯¹è¯æ¨¡æ¿æ˜¯ä¸ºäº†è®© LLM åŒºåˆ†å‡ºï¼Œ Systemã€User å’Œ Assistant
ä¸åŒçš„æ¨¡å‹ä¼šæœ‰ä¸åŒçš„æ¨¡æ¿

![å›¾ç‰‡](./imgs/å¯¹è¯æ¨¡æ¿.png)
![å›¾ç‰‡](./imgs/ä¸åŒå¯¹è¯æ¨¡æ¿.png)

èµ·å§‹ç¬¦å’Œç»“æŸç¬¦

ä¸ºäº†è®©LLMçŸ¥é“ä»€ä¹ˆæ—¶å€™å¼€å§‹å’Œç»“æŸä¸€æ®µè¯ï¼Œè®­ç»ƒæ—¶éœ€è¦å¯¹æ•°æ®æ·»åŠ èµ·å§‹ç¬¦(BOS)å’Œç»“æŸç¬¦(EOS)ï¼Œå¸¸ç”¨ $<s>$ä½œä¸ºèµ·å§‹ç¬¦  $</s>$ ä½œä¸ºç»“æŸç¬¦

å¢é‡é¢„è®­ç»ƒå¯¹è¯æ¨¡æ¿ï¼š
![å›¾ç‰‡](./imgs/å¢é‡é¢„è®­ç»ƒå¯¹è¯æ¨¡æ¿.png)

æŒ‡ä»¤è·Ÿéšå¾®è°ƒå¯¹è¯æ¨¡æ¿ï¼š
æŒ‡ä»¤è·Ÿéšå¾®è°ƒè®¡ç®—æŸå¤±åªå¯¹ç­”æ¡ˆéƒ¨åˆ†è¿›è¡Œè®¡ç®—
![å›¾ç‰‡](./imgs/æç¤ºè·Ÿéšå¯¹è¯æ¨¡æ¿.png)

## å¾®è°ƒæ–¹æ¡ˆ

### Full å…¨å‚æ•°å¾®è°ƒ
å¯¹åŸºåº§æ¨¡å‹é‡æ–°è®­ç»ƒï¼Œéœ€è¦ä¿å­˜åŸºåº§æ¨¡å‹ä¸­å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€

### LoRA
åŸºåº§æ¨¡å‹ä¸å˜ï¼Œåœ¨åŸºåº§æ¨¡å‹æ—**æ–°å¢ä¸€ä¸ªæ”¯è·¯**ï¼Œå¾®è°ƒè¿™ä¸ª **Adapter**ï¼ˆåŒ…å«ä¸¤ä¸ªè¿ç»­çš„å° Linearï¼‰
Adapter å‚æ•°é‡è¿œå°äºåŸæœ¬çš„ Linearï¼Œèƒ½å¤§å¹…é™ä½è®­ç»ƒçš„æ˜¾å­˜æ¶ˆè€—ï¼Œè®­ç»ƒæ—¶åªéœ€è¦ä¿å­˜Adapterä¸­å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€

### QLoRA
LoRAçš„æ”¹è¿›ï¼ŒåŠ è½½æ¨¡å‹æ—¶**é‡‡ç”¨äº† 4bit é‡åŒ–**ï¼Œå¹¶ä¸”ä¼˜åŒ–å™¨çŠ¶æ€åœ¨**CPUä¸GPU**é—´å¹³è¡¡
![å›¾ç‰‡](./imgs/å¾®è°ƒæ–¹æ¡ˆ.png)

## ä¼˜åŒ–æŠ€å·§

### Flash Attention

Flash Attention å°† Attention è®¡ç®—å¹¶è¡ŒåŒ–ï¼Œé¿å…è®¡ç®—è¿‡ç¨‹çš„ Attention Score NÃ—N çš„æ˜¾å­˜å ç”¨

###  DeepSpeed ç®€ä»‹

DeepSpeedæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œç”±å¾®è½¯å¼€å‘ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚å®ƒé€šè¿‡å‡ ç§å…³é”®æŠ€æœ¯æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€æ¢¯åº¦ç´¯ç§¯ã€ä»¥åŠå†…å­˜å’Œå¸¦å®½ä¼˜åŒ–ç­‰ã€‚DeepSpeedç‰¹åˆ«é€‚ç”¨äºéœ€è¦å·¨å¤§è®¡ç®—èµ„æºçš„å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†ã€‚

åœ¨DeepSpeedä¸­ï¼Œ`zero`Â ä»£è¡¨â€œZeROâ€ï¼ˆZero Redundancy Optimizerï¼‰ï¼Œæ˜¯ä¸€ç§æ—¨åœ¨é™ä½è®­ç»ƒå¤§å‹æ¨¡å‹æ‰€éœ€å†…å­˜å ç”¨çš„ä¼˜åŒ–å™¨ã€‚ZeRO é€šè¿‡ä¼˜åŒ–æ•°æ®å¹¶è¡Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œå…è®¸æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚ZeRO åˆ†ä¸ºå‡ ä¸ªä¸åŒçš„çº§åˆ«ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
-   **deepspeed_zero1**ï¼šè¿™æ˜¯ZeROçš„åŸºæœ¬ç‰ˆæœ¬ï¼Œå®ƒä¼˜åŒ–äº†æ¨¡å‹å‚æ•°çš„å­˜å‚¨ï¼Œä½¿å¾—æ¯ä¸ªGPUåªå­˜å‚¨ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œå‡å°‘å†…å­˜çš„ä½¿ç”¨ã€‚
-   **deepspeed_zero2**ï¼šåœ¨deepspeed_zero1çš„åŸºç¡€ä¸Šï¼Œdeepspeed_zero2è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å­˜å‚¨ã€‚å®ƒå°†è¿™äº›ä¿¡æ¯ä¹Ÿåˆ†æ•£åˆ°ä¸åŒçš„GPUä¸Šï¼Œè¿›ä¸€æ­¥é™ä½äº†å•ä¸ªGPUçš„å†…å­˜éœ€æ±‚ã€‚
-   **deepspeed_zero3**ï¼šè¿™æ˜¯ç›®å‰æœ€é«˜çº§çš„ä¼˜åŒ–ç­‰çº§ï¼Œå®ƒä¸ä»…åŒ…æ‹¬äº†deepspeed_zero1å’Œdeepspeed_zero2çš„ä¼˜åŒ–ï¼Œè¿˜è¿›ä¸€æ­¥å‡å°‘äº†æ¿€æ´»å‡½æ•°çš„å†…å­˜å ç”¨ã€‚è¿™é€šè¿‡åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—æ¿€æ´»ï¼ˆè€Œä¸æ˜¯å­˜å‚¨å®ƒä»¬ï¼‰æ¥å®ç°ï¼Œä»è€Œå®ç°äº†å¯¹å¤§å‹æ¨¡å‹æå…¶å†…å­˜æ•ˆç‡çš„è®­ç»ƒã€‚

é€‰æ‹©å“ªç§deepspeedç±»å‹ä¸»è¦å–å†³äºä½ çš„å…·ä½“éœ€æ±‚ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„å¤§å°ã€å¯ç”¨çš„ç¡¬ä»¶èµ„æºï¼ˆç‰¹åˆ«æ˜¯GPUå†…å­˜ï¼‰ä»¥åŠè®­ç»ƒçš„æ•ˆç‡éœ€æ±‚ã€‚ä¸€èˆ¬æ¥è¯´ï¼š

-   å¦‚æœä½ çš„**æ¨¡å‹è¾ƒå°**ï¼Œæˆ–è€…å†…å­˜èµ„æºå……è¶³ï¼Œå¯èƒ½ä¸éœ€è¦ä½¿ç”¨æœ€é«˜çº§åˆ«çš„ä¼˜åŒ–ã€‚
-   å¦‚æœä½ æ­£åœ¨å°è¯•è®­ç»ƒ**éå¸¸å¤§çš„æ¨¡å‹**ï¼Œæˆ–è€…ä½ çš„ç¡¬ä»¶èµ„æºæœ‰é™ï¼Œä½¿ç”¨**deepspeed_zero2æˆ–deepspeed_zero3**å¯èƒ½æ›´åˆé€‚ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§æ¨¡å‹çš„è®­ç»ƒã€‚
-   é€‰æ‹©æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°å®ç°çš„å¤æ‚æ€§å’Œè¿è¡Œæ—¶çš„å¼€é”€ï¼Œæ›´é«˜çº§çš„ä¼˜åŒ–å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è®¾ç½®ï¼Œå¹¶å¯èƒ½å¢åŠ ä¸€äº›è®¡ç®—å¼€é”€ã€‚

# XTuner ä»‹ç»

[XTuner GitHub](https://github.com/InternLM/xtuner/blob/main/README_zh-CN.md)

XTuner æ˜¯ä¸€ä¸ªé«˜æ•ˆã€çµæ´»ã€å…¨èƒ½çš„è½»é‡åŒ–å¤§æ¨¡å‹å¾®è°ƒå·¥å…·åº“
- æ”¯æŒ**å¤§è¯­è¨€æ¨¡å‹ LLM**ã€**å¤šæ¨¡æ€å›¾æ–‡æ¨¡å‹ VLM** çš„**é¢„è®­ç»ƒ**åŠ**è½»é‡çº§å¾®è°ƒ**ã€‚
- XTuner æ”¯æŒåœ¨ 8GB æ˜¾å­˜ä¸‹å¾®è°ƒ 7B æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå¤šèŠ‚ç‚¹è·¨è®¾å¤‡å¾®è°ƒæ›´å¤§å°ºåº¦æ¨¡å‹ï¼ˆ70B+ï¼‰
- æ”¯æŒÂ **[QLoRA](http://arxiv.org/abs/2305.14314)ã€[LoRA](http://arxiv.org/abs/2106.09685)**ã€**å…¨é‡å‚æ•°å¾®è°ƒ**ç­‰å¤šç§å¾®è°ƒç®—æ³•ï¼Œæ”¯æ’‘ç”¨æˆ·æ ¹æ®å…·ä½“éœ€æ±‚ä½œå‡ºæœ€ä¼˜é€‰æ‹©
- æ”¯æŒ**å¢é‡é¢„è®­ç»ƒ**ã€**æŒ‡ä»¤å¾®è°ƒ**ä¸ **Agent å¾®è°ƒ**

## æ”¯æŒåˆ—è¡¨
<table>
<tbody>
<tr align="center" valign="middle">
<td>
  <b>æ¨¡å‹</b>
</td>
<td>
  <b>æ•°æ®é›†</b>
</td>
<td>
  <b>æ•°æ®æ ¼å¼</b>
</td>
 <td>
  <b>å¾®è°ƒç®—æ³•</b>
</td>
</tr>
<tr valign="top">
<td align="left" valign="top">
<ul>
  <li><a href="https://huggingface.co/internlm">InternLM2</a></li>
  <li><a href="https://huggingface.co/internlm">InternLM</a></li>
  <li><a href="https://huggingface.co/meta-llama">Llama</a></li>
  <li><a href="https://huggingface.co/meta-llama">Llama2</a></li>
  <li><a href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2</a></li>
  <li><a href="https://huggingface.co/THUDM/chatglm3-6b">ChatGLM3</a></li>
  <li><a href="https://huggingface.co/Qwen/Qwen-7B">Qwen</a></li>
  <li><a href="https://huggingface.co/baichuan-inc/Baichuan-7B">Baichuan</a></li>
  <li><a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base">Baichuan2</a></li>
  <li><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral 8x7B</a></li>
  <li><a href="https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat">DeepSeek MoE</a></li>
  <li><a href="https://huggingface.co/google">Gemma</a></li>
  <li>...</li>
</ul>
</td>
<td>
<ul>
  <li><a href="https://modelscope.cn/datasets/damo/MSAgent-Bench">MSAgent-Bench</a></li>
  <li><a href="https://huggingface.co/datasets/fnlp/moss-003-sft-data">MOSS-003-SFT</a> ğŸ”§</li>
  <li><a href="https://huggingface.co/datasets/tatsu-lab/alpaca">Alpaca en</a> / <a href="https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese">zh</a></li>
  <li><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k">WizardLM</a></li>
  <li><a href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco">oasst1</a></li>
  <li><a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus">Open-Platypus</a></li>
  <li><a href="https://huggingface.co/datasets/HuggingFaceH4/CodeAlpaca_20K">Code Alpaca</a></li>
  <li><a href="https://huggingface.co/datasets/burkelibbey/colors">Colorist</a> ğŸ¨</li>
  <li><a href="https://github.com/WangRongsheng/ChatGenTitle">Arxiv GenTitle</a></li>
  <li><a href="https://github.com/LiuHC0428/LAW-GPT">Chinese Law</a></li>
  <li><a href="https://huggingface.co/datasets/Open-Orca/OpenOrca">OpenOrca</a></li>
  <li><a href="https://huggingface.co/datasets/shibing624/medical">Medical Dialogue</a></li>
  <li>...</li>
</ul>
</td>
<td>
<ul>
  <li><a href="docs/zh_cn/user_guides/incremental_pretraining.md">Incremental Pre-training</a> </li>
  <li><a href="docs/zh_cn/user_guides/single_turn_conversation.md">Single-turn Conversation SFT</a> </li>
  <li><a href="docs/zh_cn/user_guides/multi_turn_conversation.md">Multi-turn Conversation SFT</a> </li>
</ul>
</td>
<td>
<ul>
  <li><a href="http://arxiv.org/abs/2305.14314">QLoRA</a></li>
  <li><a href="http://arxiv.org/abs/2106.09685">LoRA</a></li>
  <li>å…¨é‡å‚æ•°å¾®è°ƒ</li>
</ul>
</td>
</tr>
</tbody>
</table>
## XTuner æŠ€æœ¯æ¶æ„å›¾
![å›¾ç‰‡](./imgs/XTuneræŠ€æœ¯æ¶æ„å›¾.png)

## XTuner æ“ä½œæ­¥éª¤

### 1ã€å®‰è£…
```python
pip install -U xtuner
#äº¦å¯é›†æˆ DeepSpeed å®‰è£…ï¼š
pip install -U 'xtuner[deepspeed]'
```

### 2ã€æŒ‘é€‰é…ç½®æ¨¡æ¿

[æ•°æ®é¢„å¤„ç†æŒ‡å—](https://github.com/InternLM/xtuner/blob/main/docs/zh_cn/user_guides/dataset_prepare.md)

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š
```python
xtuner list-cfg
```

![å›¾ç‰‡](./imgs/Configå‘½åè§„åˆ™.png)
![å›¾ç‰‡](./imgs/Configç¤ºä¾‹.png)

æˆ–è€…ï¼Œå¦‚æœæ‰€æä¾›çš„é…ç½®æ–‡ä»¶ä¸èƒ½æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œè¯·å¯¼å‡ºæ‰€æä¾›çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œç›¸åº”æ›´æ”¹ï¼š
```python
## æ‹·è´é…ç½®æ¨¡æ¿
xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
## ä¿®æ”¹é…ç½®æ¨¡æ¿
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
```

ä¾‹å¦‚
```python
## æ‹·è´é…ç½®æ¨¡æ¿
xtuner copy-cfg internlm_20b_qlora_oasst1_512_e3 ./
## ä¿®æ”¹é…ç½®æ¨¡æ¿
vi internlm_20b_qlora_oasst1_512_e3 _copy.py
```

### 3ã€ä¸€é”®è®­ç»ƒ

```python
xtuner train ${CONFIG_NAME_OR_PATH}
```

å¸¸ç”¨çš„è¶…å‚æ•°
![å›¾ç‰‡](./imgs/é‡è¦å‚æ•°.png)

```python
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
llm_name_or_path = '/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b'
visual_encoder_name_or_path = '/root/share/new_models/openai/clip-vit-large-patch14-336'
# Specify the pretrained pth
pretrained_pth = '/root/share/new_models/xtuner/iter_2181.pth'  # noqa: E501

# Data
data_root = '/root/tutorial/xtuner/llava/llava_data/'
data_path = data_root + 'repeated_data.json'
image_folder = data_root
prompt_template = PROMPT_TEMPLATE.internlm2_chat
max_length = int(2048 - (336 / 14)**2)
pack_to_max_length = True

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 1
dataloader_num_workers = 0
max_epochs = 1
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 500
save_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 500
SYSTEM = ''
evaluation_images = 'https://llava-vl.github.io/static/images/view.jpg'
evaluation_inputs = ['Please describe this picture','What is the equipment in the image?']
```

![å›¾ç‰‡](./imgs/æ•°æ®å¤„ç†å‡½æ•°.png)

å¦‚ä¸‹çš„ dataset_map_fn é€‰æ‹©çš„æ˜¯ llava_map_fn
```python
#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
llava_dataset = dict(
    type=LLaVADataset,
    data_path=data_path,
    image_folder=image_folder,
    tokenizer=tokenizer,
    image_processor=image_processor,
    dataset_map_fn=llava_map_fn,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    max_length=max_length,
    pad_image_to_square=True)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=llava_dataset,
    sampler=dict(
        type=LengthGroupedSampler,
        length_property='modality_length',
        per_device_batch_size=batch_size * accumulative_counts),
    collate_fn=dict(type=default_collate_fn))
```

ä¾‹å¦‚åœ¨å•å¡æˆ–å¤šå¡ä¸Šè®­ç»ƒ
```python
# å•å¡
xtuner train internlm2_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2

# å¤šå¡
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
(SLURM) srun ${SRUN_ARGS} xtuner train internlm2_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
```
-   `--deepspeed`Â è¡¨ç¤ºä½¿ç”¨Â [DeepSpeed](https://github.com/microsoft/DeepSpeed)Â æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

å°†ä¿å­˜çš„ PTH æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼š
```python
xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
```

### 4ã€å¯¹è¯

XTuner æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹è¯çš„å·¥å…·
```python
xtuner chat ${NAME_OR_PATH_TO_LLM} --adapter {NAME_OR_PATH_TO_ADAPTER} [optional arguments]
```

ä¾‹å¦‚ä¸ InternLM2-Chat-7B, oasst1 adapter å¯¹è¯ï¼š
```python
xtuner chat internlm/internlm2-chat-7b --adapter xtuner/internlm2-chat-7b-qlora-oasst1 --prompt-template internlm2_chat
```

4bit æ¨¡å‹å¯¹è¯
```python
xtuner chat internlm/internlm2-chat-7b --bits 4
```

### 5ã€éƒ¨ç½²

**æ­¥éª¤ 0**ï¼Œå°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼š
```python
xtuner convert merge \
    ${NAME_OR_PATH_TO_LLM} \
    ${NAME_OR_PATH_TO_ADAPTER} \
    ${SAVE_PATH} \
    --max-shard-size 2GB
```

**æ­¥éª¤ 1**ï¼Œä½¿ç”¨ä»»æ„æ¨ç†æ¡†æ¶éƒ¨ç½²å¾®è°ƒåçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚Â [LMDeploy](https://github.com/InternLM/lmdeploy)Â ï¼š
```python
pip install lmdeploy
python -m lmdeploy.pytorch.chat ${NAME_OR_PATH_TO_LLM} \
    --max_new_tokens 256 \
    --temperture 0.8 \
    --top_p 0.95 \
    --seed 0
```

è¿½æ±‚é€Ÿåº¦æ›´å¿«ã€æ˜¾å­˜å ç”¨æ›´ä½çš„æ¨ç†ï¼Ÿæ¬¢è¿ä½“éªŒÂ [LMDeploy](https://github.com/InternLM/lmdeploy)Â æä¾›çš„ 4-bit é‡åŒ–ï¼ä½¿ç”¨æŒ‡å—è¯·è§[æ–‡æ¡£](https://github.com/InternLM/lmdeploy/tree/main#quantization)ã€‚

# XTuner å¾®è°ƒä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥
[XTuner å¾®è°ƒä¸ªäººå°åŠ©æ‰‹](https://github.com/InternLM/Tutorial/blob/camp2/xtuner/personal_assistant_document.md)

## ç¯å¢ƒå®‰è£…

```python
pip install -U 'xtuner==0.1.17[deepspeed]'
```

## å‰æœŸå‡†å¤‡

![å›¾ç‰‡](./imgs/å‰æœŸå‡†å¤‡.png)

### **æ•°æ®é›†å‡†å¤‡**

```python
## åˆ›å»ºå­˜æ”¾æ•°æ®çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/data  && cd /root/ft/data
```

ä¹‹åæˆ‘ä»¬å¯ä»¥åœ¨Â `data`Â ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªÂ `generate_data.py`Â æ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶å°†äº§ç”Ÿæ•°æ®é›† `personal_assistant.json`

```python
import json
# è®¾ç½®ç”¨æˆ·çš„åå­—
name = 'shane'
# è®¾ç½®éœ€è¦é‡å¤æ·»åŠ çš„æ•°æ®æ¬¡æ•°
n =  10000
# åˆå§‹åŒ–OpenAIæ ¼å¼çš„æ•°æ®ç»“æ„
data = [
    {
        "messages": [
            {
                "role": "user",
                "content": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»"
            },
            {
                "role": "assistant",
                "content": "æˆ‘æ˜¯{}çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„1.8Bå¤§æ¨¡å‹å“¦".format(name)
            }
        ]
    }
]

# é€šè¿‡å¾ªç¯ï¼Œå°†åˆå§‹åŒ–çš„å¯¹è¯æ•°æ®é‡å¤æ·»åŠ åˆ°dataåˆ—è¡¨ä¸­
for i in range(n):
    data.append(data[0])

# å°†dataåˆ—è¡¨ä¸­çš„æ•°æ®å†™å…¥åˆ°ä¸€ä¸ªåä¸º'personal_assistant.json'çš„æ–‡ä»¶ä¸­
with open('personal_assistant.json', 'w', encoding='utf-8') as f:
    # ä½¿ç”¨json.dumpæ–¹æ³•å°†æ•°æ®ä»¥JSONæ ¼å¼å†™å…¥æ–‡ä»¶
    # ensure_ascii=False ç¡®ä¿ä¸­æ–‡å­—ç¬¦æ­£å¸¸æ˜¾ç¤º
    # indent=4 ä½¿å¾—æ–‡ä»¶å†…å®¹æ ¼å¼åŒ–ï¼Œä¾¿äºé˜…è¯»
    json.dump(data, f, ensure_ascii=False, indent=4)
```

å¯ä»¥çœ‹åˆ°åœ¨dataçš„è·¯å¾„ä¸‹ä¾¿ç”Ÿæˆäº†ä¸€ä¸ªåä¸ºÂ `personal_assistant.json`Â çš„æ–‡ä»¶ï¼Œè¿™æ ·æˆ‘ä»¬æœ€å¯ç”¨äºå¾®è°ƒçš„æ•°æ®é›†å°±å‡†å¤‡å¥½å•¦ï¼

**æ–‡ä»¶ç»“æ„**
```
|-- data/
    |-- personal_assistant.json
    |-- generate_data.py
```

### **æ¨¡å‹å‡†å¤‡**

ä½¿ç”¨ InternLM æœ€æ–°æ¨å‡ºçš„å°æ¨¡å‹Â `InterLM2-Chat-1.8B`Â æ¥å®Œæˆæ­¤æ¬¡çš„å¾®è°ƒæ¼”ç¤º

```python
# -pé€‰é¡¹æ„å‘³ç€å¦‚æœä¸Šçº§ç›®å½•ä¸å­˜åœ¨ä¹Ÿä¼šä¸€å¹¶åˆ›å»ºï¼Œä¸”å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨åˆ™ä¸ä¼šæŠ¥é”™ã€‚
mkdir -p /root/ft/model
# å¤åˆ¶å†…å®¹åˆ°ç›®æ ‡æ–‡ä»¶å¤¹ã€‚-ré€‰é¡¹è¡¨ç¤ºé€’å½’å¤åˆ¶æ•´ä¸ªæ–‡ä»¶å¤¹ã€‚
cp -r /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/* /root/ft/model/
```

å‡å¦‚å­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®é€šè¿‡ç¬¦å·é“¾æ¥çš„æ–¹å¼é“¾æ¥åˆ°æ¨¡å‹æ–‡ä»¶

```python
# åˆ›å»ºç¬¦å·é“¾æ¥
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/ft/model
```
æ‰§è¡Œä¸Šè¿°æ“ä½œåï¼Œ`/root/ft/model`Â å°†ç›´æ¥æˆä¸ºä¸€ä¸ªç¬¦å·é“¾æ¥ï¼Œè¿™ä¸ªé“¾æ¥æŒ‡å‘Â `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`Â çš„ä½ç½®

**æ–‡ä»¶ç»“æ„**
```txt
|-- model/
    |-- tokenizer.model
    |-- config.json
    |-- .mdl
    |-- tokenization_internlm2.py
    |-- model-00002-of-00002.safetensors
    |-- tokenizer_config.json
    |-- model-00001-of-00002.safetensors
    |-- model.safetensors.index.json
    |-- configuration.json
    |-- .msc
    |-- special_tokens_map.json
    |-- .mv
    |-- modeling_internlm2.py
    |-- README.md
    |-- configuration_internlm2.py
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```

### **é…ç½®æ–‡ä»¶é€‰æ‹©**

åœ¨å‡†å¤‡å¥½äº†**æ¨¡å‹**å’Œ**æ•°æ®é›†**åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„**å¾®è°ƒæ–¹æ³•**ç»“åˆå‰é¢çš„ä¿¡æ¯æ¥æ‰¾åˆ°ä¸æˆ‘ä»¬**æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶**äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚
æ‰€è°“é…ç½®æ–‡ä»¶ï¼ˆconfigï¼‰ï¼Œå…¶å®æ˜¯ä¸€ç§ç”¨äºå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­å„ä¸ªæ–¹é¢çš„å‚æ•°å’Œè®¾ç½®çš„å·¥å…·ã€‚

```python
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®æ–‡ä»¶
xtuner list-cfg
# å‡å¦‚æˆ‘ä»¬æƒ³æ‰¾åˆ° internlm2-1.8b æ¨¡å‹é‡Œæ”¯æŒçš„é…ç½®æ–‡ä»¶
xtuner list-cfg -p internlm2_1_8b
```

```
==========================CONFIGS===========================
PATTERN: internlm2_1_8b
-------------------------------
internlm2_1_8b_full_alpaca_e3
internlm2_1_8b_qlora_alpaca_e3
=============================================================
```

è™½ç„¶æˆ‘ä»¬ç”¨çš„æ•°æ®é›†å¹¶ä¸æ˜¯Â `alpaca`Â è€Œæ˜¯æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬åˆ¶ä½œçš„å°åŠ©æ‰‹æ•°æ®é›† ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬æ˜¯é€šè¿‡Â `QLoRA`Â çš„æ–¹å¼å¯¹Â `internlm2-chat-1.8b`Â è¿›è¡Œå¾®è°ƒã€‚è€Œæœ€ç›¸è¿‘çš„é…ç½®æ–‡ä»¶åº”è¯¥å°±æ˜¯Â `internlm2_1_8b_qlora_alpaca_e3`Â ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ‹·è´è¿™ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•

```python
# åˆ›å»ºä¸€ä¸ªå­˜æ”¾ config æ–‡ä»¶çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/config

# ä½¿ç”¨ XTuner ä¸­çš„ copy-cfg åŠŸèƒ½å°† config æ–‡ä»¶å¤åˆ¶åˆ°æŒ‡å®šçš„ä½ç½®
xtuner copy-cfg internlm2_1_8b_qlora_alpaca_e3 /root/ft/config
```

**æ–‡ä»¶ç»“æ„**
```txt
|-- config/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
```

### å°ç»“

æ–‡ä»¶ç»“æ„
```txt
|-- ft/
    |-- config/
        |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- model/
        |-- tokenizer.model
        |-- config.json
        |-- tokenization_internlm2.py
        |-- model-00002-of-00002.safetensors
        |-- tokenizer_config.json
        |-- model-00001-of-00002.safetensors
        |-- model.safetensors.index.json
        |-- configuration.json
        |-- special_tokens_map.json
        |-- modeling_internlm2.py
        |-- README.md
        |-- configuration_internlm2.py
        |-- generation_config.json
        |-- tokenization_internlm2_fast.py
    |-- data/
        |-- personal_assistant.json
        |-- generate_data.py
```


## é…ç½®æ–‡ä»¶ä¿®æ”¹

![å›¾ç‰‡](./imgs/é…ç½®æ–‡ä»¶å‡†å¤‡.png)

åœ¨é€‰æ‹©äº†ä¸€ä¸ªæœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶å¹¶å‡†å¤‡å¥½å…¶ä»–å†…å®¹åï¼Œä¸‹é¢æˆ‘ä»¬è¦åšçš„äº‹æƒ…å°±æ˜¯æ ¹æ®æˆ‘ä»¬è‡ªå·±çš„å†…å®¹å¯¹è¯¥é…ç½®æ–‡ä»¶è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬å®é™…è®­ç»ƒçš„è¦æ±‚ã€‚
å°† `internlm2_1_8b_qlora_alpaca_e3_copy.py` æ–‡ä»¶å†…å®¹ä¿®æ”¹ä¸º

ä¿®æ”¹æ¨¡å‹åŠæ•°æ®é›†åœ°å€
```python
pretrained_model_name_or_path = '/root/ft/model'
alpaca_en_path = '/root/ft/data/personal_assistant.json'
```

prompt_template é€‰æ‹©
```python
prompt_template = PROMPT_TEMPLATE.default
```


```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch
from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
from torch.optim import AdamW
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import openai_map_fn, template_map_fn_factory
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune
from xtuner.parallel.sequence import SequenceParallelSampler
from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = '/root/ft/model'
use_varlen_attn = False

# Data
alpaca_en_path = '/root/ft/data/personal_assistant.json'
prompt_template = PROMPT_TEMPLATE.default
max_length = 1024
pack_to_max_length = True

# parallel
sequence_parallel_size = 1

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 16
accumulative_counts *= sequence_parallel_size
dataloader_num_workers = 0
max_epochs = 2
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 300
save_total_limit = 3  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 300
SYSTEM = ''
evaluation_inputs = ['è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ æ˜¯è°', 'ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—']

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM'))

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=openai_map_fn,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

sampler = SequenceParallelSampler \
    if sequence_parallel_size > 1 else DefaultSampler
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=alpaca_en,
    sampler=dict(type=sampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = None

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)
```

## æ¨¡å‹è®­ç»ƒ

![å›¾ç‰‡](./imgs/æ¨¡å‹è®­ç»ƒ.png)

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ Â `--work-dir`Â æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®

```python
# æŒ‡å®šä¿å­˜è·¯å¾„
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train
```

**æ–‡ä»¶ç»“æ„**
```txt
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
```

### DeepSpeed åŠ é€Ÿ

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»“åˆ XTuner å†…ç½®çš„Â `deepspeed`Â æ¥åŠ é€Ÿæ•´ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…±æœ‰ä¸‰ç§ä¸åŒçš„Â `deepspeed`Â ç±»å‹å¯è¿›è¡Œé€‰æ‹©ï¼Œåˆ†åˆ«æ˜¯Â `deepspeed_zero1`,Â `deepspeed_zero2`Â å’ŒÂ `deepspeed_zero3`

```python
# ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train_deepspeed --deepspeed deepspeed_zero2
```

é€šè¿‡Â `deepspeed`Â æ¥è®­ç»ƒåå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’ŒåŸæœ¬çš„æƒé‡æ–‡ä»¶æ˜¯æœ‰æ‰€å·®åˆ«çš„ï¼ŒåŸæœ¬çš„ä»…ä»…æ˜¯ä¸€ä¸ª .pth çš„æ–‡ä»¶ï¼Œè€Œä½¿ç”¨äº†Â `deepspeed`Â åˆ™æ˜¯ä¸€ä¸ªåå­—å¸¦æœ‰ .pth çš„æ–‡ä»¶å¤¹ï¼Œåœ¨è¯¥æ–‡ä»¶å¤¹é‡Œä¿å­˜äº†ä¸¤ä¸ª .pt æ–‡ä»¶ã€‚

DeepSpeed åŠ é€Ÿæ–‡ä»¶ç»“æ„
```
|-- train_deepspeed/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- zero_to_fp32.py
    |-- last_checkpoint
    |-- iter_600.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
    |-- 20240406_220727/
        |-- 20240406_220727.log
        |-- vis_data/
            |-- 20240406_220727.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
    |-- iter_768.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
    |-- iter_300.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
```

### **æ¨¡å‹ç»­è®­**

å‡å¦‚æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çªç„¶è¢«ä¸­æ–­äº†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡åœ¨åŸæœ‰æŒ‡ä»¤çš„åŸºç¡€ä¸ŠåŠ ä¸ŠÂ `--resume {checkpoint_path}`Â æ¥å®ç°æ¨¡å‹çš„ç»§ç»­è®­ç»ƒã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªç»§ç»­è®­ç»ƒå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’Œä¸­æ–­å‰çš„å®Œå…¨ä¸€è‡´ï¼Œå¹¶ä¸ä¼šæœ‰ä»»ä½•åŒºåˆ«ã€‚ä¸‹é¢æˆ‘å°†ç”¨è®­ç»ƒäº†500è½®çš„ä¾‹å­æ¥è¿›è¡Œæ¼”ç¤ºã€‚

```python
# æ¨¡å‹ç»­è®­
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train --resume /root/ft/train/iter_600.pth
```
è™½ç„¶æƒé‡æ–‡ä»¶å¹¶æ²¡æœ‰å‘ç”Ÿæ”¹å˜ï¼Œä½†æ˜¯ä¼šå¤šä¸€ä¸ªä»¥æ—¶é—´æˆ³ä¸ºåçš„è®­ç»ƒè¿‡ç¨‹æ–‡ä»¶å¤¹ä¿å­˜è®­ç»ƒçš„è¿‡ç¨‹æ•°æ®ã€‚

æ–‡ä»¶ç»“æ„
```
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
    |-- 20240406_225723/
        |-- 20240406_225723.log
        |-- vis_data/
            |-- 20240406_225723.json
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- config.py
```

## æ¨¡å‹è½¬æ¢ã€æ•´åˆã€æµ‹è¯•åŠéƒ¨ç½²

![å›¾ç‰‡](./imgs/æ¨¡å‹è½¬æ¢.png)

### **æ¨¡å‹è½¬æ¢**

ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ Huggingface æ ¼å¼æ–‡ä»¶

```python
# åˆ›å»ºä¸€ä¸ªä¿å­˜è½¬æ¢å Huggingface æ ¼å¼çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/huggingface

# æ¨¡å‹è½¬æ¢
# xtuner convert pth_to_hf ${é…ç½®æ–‡ä»¶åœ°å€} ${æƒé‡æ–‡ä»¶åœ°å€} ${è½¬æ¢åæ¨¡å‹ä¿å­˜åœ°å€}
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/iter_768.pth /root/ft/huggingface
```

æ–‡ä»¶ç»“æ„
```
|-- huggingface/
    |-- adapter_config.json
    |-- xtuner_config.py
    |-- adapter_model.bin
    |-- README.md
```

**æ­¤æ—¶ï¼Œhuggingface æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**
LoRA æ¨¡å‹æ–‡ä»¶ = Adapter

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨è½¬æ¢çš„æŒ‡ä»¤ä¸­æ·»åŠ å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªï¼š

	--fp32  ä»£è¡¨ä»¥fp32çš„ç²¾åº¦å¼€å¯ï¼Œå‡å¦‚ä¸è¾“å…¥åˆ™é»˜è®¤ä¸ºfp16
	--max-shard-size {GB}  ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰

### **æ¨¡å‹æ•´åˆ**

å¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆadapterï¼‰ã€‚é‚£ä¹ˆè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œç»„åˆæ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚

åœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªåœ°å€ï¼ŒåŒ…æ‹¬**åŸæ¨¡å‹çš„åœ°å€**ã€è®­ç»ƒå¥½çš„ **adapter å±‚çš„åœ°å€**ï¼ˆè½¬ä¸º Huggingface æ ¼å¼åä¿å­˜çš„éƒ¨åˆ†ï¼‰ä»¥åŠ**æœ€ç»ˆä¿å­˜çš„åœ°å€**ã€‚

```python
# åˆ›å»ºä¸€ä¸ªåä¸º final_model çš„æ–‡ä»¶å¤¹å­˜å‚¨æ•´åˆåçš„æ¨¡å‹æ–‡ä»¶
mkdir -p /root/ft/final_model

# è§£å†³ä¸€ä¸‹çº¿ç¨‹å†²çªçš„ Bug 
export MKL_SERVICE_FORCE_INTEL=1

# è¿›è¡Œæ¨¡å‹æ•´åˆ
# xtuner convert merge  ${NAME_OR_PATH_TO_LLM} ${NAME_OR_PATH_TO_ADAPTER} ${SAVE_PATH} 
xtuner convert merge /root/ft/model /root/ft/huggingface /root/ft/final_model
```

æ•´åˆåçš„æ–‡ä»¶ç»“æ„
```txt
|-- final_model/
    |-- tokenizer.model
    |-- config.json
    |-- pytorch_model.bin.index.json
    |-- pytorch_model-00001-of-00002.bin
    |-- tokenization_internlm2.py
    |-- tokenizer_config.json
    |-- special_tokens_map.json
    |-- pytorch_model-00002-of-00002.bin
    |-- modeling_internlm2.py
    |-- configuration_internlm2.py
    |-- tokenizer.json
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```

### **å¯¹è¯æµ‹è¯•**

åœ¨ XTuner ä¸­ä¹Ÿç›´æ¥çš„æä¾›äº†ä¸€å¥—åŸºäº transformers çš„å¯¹è¯ä»£ç ï¼Œè®©æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ç»ˆç«¯ä¸ Huggingface æ ¼å¼çš„æ¨¡å‹è¿›è¡Œå¯¹è¯æ“ä½œã€‚
æˆ‘ä»¬åªéœ€è¦å‡†å¤‡æˆ‘ä»¬åˆšåˆšè½¬æ¢å¥½çš„æ¨¡å‹è·¯å¾„å¹¶é€‰æ‹©å¯¹åº”çš„æç¤ºè¯æ¨¡ç‰ˆï¼ˆprompt-templateï¼‰å³å¯è¿›è¡Œå¯¹è¯ã€‚

æƒ³è¦äº†è§£å…·ä½“æ¨¡å‹çš„ prompt-template æˆ–è€… XTuner é‡Œæ”¯æŒçš„ prompt-tempolateï¼Œå¯ä»¥åˆ° XTuner æºç ä¸­çš„Â `xtuner/utils/templates.py`Â è¿™ä¸ªæ–‡ä»¶ä¸­è¿›è¡ŒæŸ¥æ‰¾ã€‚

```python
# Copyright (c) OpenMMLab. All rights reserved.
from mmengine.config import ConfigDict

# - Turn 0: SYSTEM + INSTRUCTION, [output + SUFFIX], SEP
# - Turn 1: INSTRUCTION, [output + SUFFIX], SEP
# - Turn ...
# Note: [] means having supervised loss during the fine-tuning
PROMPT_TEMPLATE = ConfigDict(
    default=dict(
        SYSTEM='<|System|>:{system}\n',
        INSTRUCTION='<|User|>:{input}\n<|Bot|>:',
        SEP='\n'),
    zephyr=dict(
        SYSTEM='<|system|>\n{system}\n',
        INSTRUCTION='<|user|>\n{input}\n<|assistant|>\n',
        SEP='\n'),
    internlm_chat=dict(
        SYSTEM='<|System|>:{system}\n',
        INSTRUCTION='<|User|>:{input}<eoh>\n<|Bot|>:',
        SUFFIX='<eoa>',
        SUFFIX_AS_EOS=True,
        SEP='\n',
        STOP_WORDS=['<eoa>']),
    internlm2_chat=dict(
        SYSTEM='<|im_start|>system\n{system}<|im_end|>\n',
        INSTRUCTION=('<|im_start|>user\n{input}<|im_end|>\n'
                     '<|im_start|>assistant\n'),
        SUFFIX='<|im_end|>',
        SUFFIX_AS_EOS=True,
        SEP='\n',
        STOP_WORDS=['<|im_end|>']),
    moss_sft=dict(
        SYSTEM='{system}\n',
        INSTRUCTION='<|Human|>: {input}<eoh>\n',
        SEP='\n',
        STOP_WORDS=['<eoc>', '<eom>']),
    llama2_chat=dict(
        SYSTEM=(
            '[INST] <<SYS>>\n You are a helpful, respectful and honest '
            'assistant. Always answer as helpfully as possible, while being '
            'safe. Your answers should not include any harmful, unethical, '
            'racist, sexist, toxic, dangerous, or illegal content. Please '
            'ensure that your responses are socially unbiased and positive in '
            'nature.\n{system}\n<</SYS>>\n [/INST] '),
        INSTRUCTION='[INST] {input} [/INST]',
        SEP='\n'),
    code_llama_chat=dict(
        SYSTEM='{system}\n', INSTRUCTION='[INST] {input} [/INST]'),
    chatglm2=dict(
        SYSTEM='{system}\n',
        INSTRUCTION='[Round {round}]\n\né—®ï¼š{input}\n\nç­”ï¼š',
        SEP='\n\n'),
    chatglm3=dict(
        SYSTEM='<|system|>\n{system}',
        INSTRUCTION='<|user|>\n{input}<|assistant|>\n',
        SEP='\n'),
    qwen_chat=dict(
        SYSTEM=('<|im_start|>system\n{system}<|im_end|>\n'),
        INSTRUCTION=('<|im_start|>user\n{input}<|im_end|>\n'
                     '<|im_start|>assistant\n'),
        SUFFIX='<|im_end|>',
        SUFFIX_AS_EOS=True,
        SEP='\n',
        STOP_WORDS=['<|im_end|>', '<|endoftext|>']),
    baichuan_chat=dict(
        SYSTEM='{system}\n',
        INSTRUCTION='<reserved_102>{input}<reserved_103>',
        SEP='\n'),
    baichuan2_chat=dict(
        SYSTEM='{system}\n',
        INSTRUCTION='<reserved_106>{input}<reserved_107>',
        SEP='\n'),
    wizardlm=dict(
        SYSTEM=('A chat between a curious user and an artificial '
                'intelligence assistant. The assistant gives '
                'helpful, detailed, and polite answers to the '
                'user\'s questions. {system}\n '),
        INSTRUCTION=('USER: {input} ASSISTANT:'),
        SEP='\n'),
    wizardcoder=dict(
        SYSTEM=(
            'Below is an instruction that describes a task. '
            'Write a response that appropriately completes the request.\n\n'
            '{system}\n '),
        INSTRUCTION=('### Instruction:\n{input}\n\n### Response:'),
        SEP='\n\n'),
    vicuna=dict(
        SYSTEM=('A chat between a curious user and an artificial '
                'intelligence assistant. The assistant gives '
                'helpful, detailed, and polite answers to the '
                'user\'s questions. {system}\n '),
        INSTRUCTION=('USER: {input} ASSISTANT:'),
        SEP='\n'),
    deepseek_coder=dict(
        SYSTEM=('You are an AI programming assistant, utilizing '
                'the DeepSeek Coder model, developed by DeepSeek'
                'Company, and you only answer questions related '
                'to computer science. For politically sensitive '
                'questions, security and privacy issues, and '
                'other non-computer science questions, you will '
                'refuse to answer. {system}\n'),
        INSTRUCTION=('### Instruction:\n{input}\n### Response:\n'),
        SEP='\n'),
    # TODO: deprecation, v0.2.0
    deepseekcoder=dict(
        SYSTEM=('You are an AI programming assistant, utilizing '
                'the DeepSeek Coder model, developed by DeepSeek'
                'Company, and you only answer questions related '
                'to computer science. For politically sensitive '
                'questions, security and privacy issues, and '
                'other non-computer science questions, you will '
                'refuse to answer. {system}\n'),
        INSTRUCTION=('### Instruction:\n{input}\n### Response:\n'),
        SEP='\n'),
    deepseek_moe=dict(
        SYSTEM=('[INST] {system} [/INST]\n'),
        INSTRUCTION=('[INST] {input} [/INST]'),
        SEP='\n'),
    mistral=dict(
        SYSTEM=('[INST] {system} [/INST]\n'),
        INSTRUCTION=('[INST] {input} [/INST]'),
        SEP='\n'),
    mixtral=dict(
        SYSTEM=('[INST] {system} [/INST]\n'),
        INSTRUCTION=('[INST] {input} [/INST]'),
        SEP='\n'),
    gemma=dict(
        # `system` field is extended by xtuner
        SYSTEM=('<start_of_turn>system\n{system}<end_of_turn>\n'),
        INSTRUCTION=('<start_of_turn>user\n{input}<end_of_turn>\n'
                     '<start_of_turn>model\n'),
        SUFFIX='<end_of_turn>',
        SUFFIX_AS_EOS=False,
        SEP='\n',
        STOP_WORDS=['<end_of_turn>']),
)

SYSTEM_TEMPLATE = ConfigDict(
    moss_sft=('You are an AI assistant whose name is {bot_name}.\n'
              'Capabilities and tools that {bot_name} can possess.\n'
              '- Inner thoughts: enabled.\n'
              '- Web search: enabled. API: Search(query)\n'
              '- Calculator: enabled. API: Calculate(expression)\n'
              '- Equation solver: enabled. API: Solve(equation)\n'
              '- Text-to-image: disabled.\n'
              '- Image edition: disabled.\n'
              '- Text-to-speech: disabled.\n'),
    alpaca=('Below is an instruction that describes a task. '
            'Write a response that appropriately completes the request.\n'),
    arxiv_gentile=('If you are an expert in writing papers, please generate '
                   "a good paper title for this paper based on other authors' "
                   'descriptions of their abstracts.\n'),
    colorist=('You are a professional color designer. Please provide the '
              'corresponding colors based on the description of Human.\n'),
    coder=('You are a professional programer. Please provide the '
           'corresponding code based on the description of Human.\n'),
    lawyer='ä½ ç°åœ¨æ˜¯ä¸€åä¸“ä¸šçš„ä¸­å›½å¾‹å¸ˆï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå‡†ç¡®ã€æœ‰ç†æœ‰æ®çš„å›å¤ã€‚\n',
    medical='å¦‚æœä½ æ˜¯ä¸€ååŒ»ç”Ÿï¼Œè¯·æ ¹æ®æ‚£è€…çš„æè¿°å›ç­”åŒ»å­¦é—®é¢˜ã€‚\n',
    sql=('If you are an expert in SQL, please generate a good SQL Query '
         'for Question based on the CREATE TABLE statement.\n'),
)
```

```python
# ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/final_model --prompt-template internlm2_chat
# åŒæ ·çš„æˆ‘ä»¬ä¹Ÿå¯ä»¥å’ŒåŸæ¨¡å‹è¿›è¡Œå¯¹è¯è¿›è¡Œå¯¹æ¯”
xtuner chat /root/ft/model --prompt-template internlm2_chat
```

![å›¾ç‰‡](./imgs/xtuner-chatå‚æ•°.png)

é™¤äº†è¿™äº›å‚æ•°ä»¥å¤–å…¶å®è¿˜æœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°å°±æ˜¯Â `--adapter`Â ï¼Œè¿™ä¸ªå‚æ•°ä¸»è¦çš„ä½œç”¨å°±æ˜¯å¯ä»¥åœ¨è½¬åŒ–åçš„ adapter å±‚ä¸åŸæ¨¡å‹æ•´åˆä¹‹å‰æ¥å¯¹è¯¥å±‚è¿›è¡Œæµ‹è¯•ã€‚

```python
# ä½¿ç”¨ --adapter å‚æ•°ä¸å®Œæ•´çš„æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/model --adapter /root/ft/huggingface --prompt-template internlm2_chat
```

ä½¿ç”¨è¿™ä¸ª**é¢å¤–çš„å‚æ•°å¯¹è¯çš„æ¨¡å‹å’Œæ•´åˆåçš„æ¨¡å‹å‡ ä¹æ²¡æœ‰ä»€ä¹ˆå¤ªå¤šçš„åŒºåˆ«**ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡æµ‹è¯•ä¸åŒçš„æƒé‡æ–‡ä»¶ç”Ÿæˆçš„ adapter æ¥**æ‰¾åˆ°æœ€ä¼˜çš„ adapter** è¿›è¡Œæœ€ç»ˆçš„æ¨¡å‹æ•´åˆå·¥ä½œã€‚

## æœ€ç»ˆå¾—åˆ°æ•ˆæœ

![å›¾ç‰‡](./imgs/å¯¹è¯æµ‹è¯•.png)

## WEB DEMO éƒ¨ç½²

![å›¾ç‰‡](./imgs/webéƒ¨ç½².png)

# XTunerå¤šæ¨¡æ€è®­ç»ƒä¸æµ‹è¯•

è®­ç»ƒå‰

![å›¾ç‰‡](./imgs/LLaVAè®­ç»ƒå‰.png)

è®­ç»ƒå
![å›¾ç‰‡](./imgs/LLaVAè®­ç»ƒå.png)